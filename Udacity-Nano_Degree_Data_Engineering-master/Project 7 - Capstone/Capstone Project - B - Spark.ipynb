{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Travels : Business / Pleasure / Education / All\n",
    "### Data Engineering Capstone Project ( Spark )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Note to self : Remove all count operations when copied to python programs as its lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<div style=\"background-color:#ffe5e5;\">\n",
    "<br>    \n",
    "    <b>Important</b> : No need to execute this as it will overwrite <code>./outputs/</code> &amp; <code>./outputs-gzip/dfs_ids1.gzip</code>\n",
    "<br>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy[speedup] in /opt/conda/lib/python3.6/site-packages (0.18.0)\n",
      "Requirement already satisfied: python-levenshtein>=0.12; extra == \"speedup\" in /opt/conda/lib/python3.6/site-packages (from fuzzywuzzy[speedup]) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from python-levenshtein>=0.12; extra == \"speedup\"->fuzzywuzzy[speedup]) (38.4.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Below used by fuzzy matching\n",
    "!{sys.executable} -m pip install fuzzywuzzy[speedup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "#import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sql_queries import *\n",
    "import datetime as dt\n",
    "import json\n",
    "from fuzzywuzzy import fuzz, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, TimestampType, LongType, DateType, NullType\n",
    "\n",
    "import datetime #Required for ts conversion\n",
    "#from pyspark.sql.functions import udf\n",
    "#from pyspark.sql import functions as F\n",
    "\n",
    "#import pyspark.sql.functions as F\n",
    "\n",
    "#from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
    "\n",
    "#from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf, lit, datediff, when, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', '164d3a7415ec'),\n",
       " ('spark.app.id', 'local-1581749391129'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/root/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/root/.ivy2/jars/com.epam_parso-2.0.8.jar,/root/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///root/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///root/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.files',\n",
       "  'file:///root/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///root/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.driver.port', '41955'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.jars',\n",
       "  'file:///root/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///root/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///root/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///root/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://164d3a7415ec:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f967a7207b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Panda options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# for all rows set to None\n",
    "# pd.set_option('display.max_rows', None) \n",
    "\n",
    "pd.set_option('display.max_rows', 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Below are some prerequisite before cleaning i94 Immigration\n",
    "\n",
    "##### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def strip_all_columns(df):\n",
    "    \"\"\"\n",
    "    Summary line.\n",
    "    Strip all columns in a dataframe\n",
    "    \n",
    "    Parameters:\n",
    "    arg1 (dataframe)\n",
    "    \n",
    "    Returns:\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    for colu in df.columns:\n",
    "        if df[colu].dtype == 'object':            \n",
    "            #print('COL = ',col,' - ',df[col].dtype)\n",
    "            #df[col] = df[col].str.strip()           \n",
    "            mask = df[colu].notnull()\n",
    "            df.loc[mask, colu] = df.loc[mask, colu].map(str.strip)            \n",
    "            #df[col] = df[col].map(str.strip)\n",
    "    return df\n",
    "\n",
    "def difference (list1, list2): \n",
    "    \"\"\"\n",
    "    Summary line. \n",
    "    Difference between two lists used to compare list of two sets of columns\n",
    "\n",
    "    Parameters: \n",
    "    arg1 (list 1)\n",
    "    arg2 (list 2)\n",
    "\n",
    "    Returns: \n",
    "    list with difference\n",
    "    \n",
    "    Sample : print(difference(keep_columns, df_jan.columns))\n",
    "    \"\"\"                \n",
    "    list_dif = [i for i in list1 + list2 if i not in list1 or i not in list2]\n",
    "    return list_dif\n",
    "\n",
    "def fuzzymatch_city_get_ratio(row):\n",
    "    \"\"\"\n",
    "    Summary line.\n",
    "    Match city with US Cities list\n",
    "    \n",
    "    Parameters:\n",
    "    arg1 (dataframe row)\n",
    "    \n",
    "    Returns:\n",
    "    Panda series containing matched city name & score\n",
    "    \"\"\"\n",
    "    city, state_code = row['City'], row['State Code']\n",
    "    #print('City = ', city, 'State code = ',state_code)\n",
    "    cities = df_uszips.loc[df_uszips.state_id==state_code, 'city'].str.lower().unique()    \n",
    "    #print(process.extractOne(city, cities, scorer=fuzz.ratio))\n",
    "    return pd.Series(process.extractOne(city, cities, scorer=fuzz.ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEXICO Air Sea, and Not Reported (I-94, no land arrivals)\n",
      "HONOLULU, HI          \n"
     ]
    }
   ],
   "source": [
    "# JSON to dictionary\n",
    "fname = 'inputs/i94cit.json'\n",
    "i94cit = json.load(open(fname))\n",
    "print(i94cit[\"582\"])\n",
    "\n",
    "fname = 'inputs/i94port.json'\n",
    "i94port = json.load(open(fname))\n",
    "print(i94port[\"HHW\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_i94countrycode = 289\n",
      "df_USPoE = 660\n"
     ]
    }
   ],
   "source": [
    "# Airport Code Table\n",
    "df_ac = pd.read_csv('inputs/airport-codes_csv.csv')\n",
    "\n",
    "# Convert JSON Dictionaries to Dataframes\n",
    "df_i94countrycode = pd.DataFrame(list(i94cit.items()), columns=['code', 'country'])\n",
    "print('df_i94countrycode = {}'.format(df_i94countrycode.code.value_counts().shape[0]))\n",
    "\n",
    "df_USPoE = pd.DataFrame(list(i94port.items()), columns=['code', 'citystate'])\n",
    "print('df_USPoE = {}'.format(df_USPoE.code.value_counts().shape[0]))\n",
    "\n",
    "# International Airports\n",
    "df_iap = pd.read_csv('inputs/InternationalAirports.csv', sep=',', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Visa Type\n",
    "df_visatype = pd.read_csv('inputs/visatype2.csv', sep='|')\n",
    "\n",
    "# U.S. City Demographic Data\n",
    "df_uscd = pd.read_csv('inputs/us-cities-demographics.csv', sep=';')\n",
    "\n",
    "# World Temperature Data\n",
    "df_temper = pd.read_csv('inputs/world_temperature.csv')\n",
    "\n",
    "# Airliner Codes - Can be used join airline column in i94-immigration data\n",
    "df_alc = pd.read_csv('inputs/airline-codes.csv')\n",
    "\n",
    "# Can be used to map to df_ids.visapost column\n",
    "df_visapost = pd.read_csv('inputs/visapost.csv')\n",
    "\n",
    "# US zip-codes\n",
    "df_uszips = pd.read_csv('inputs/uszips.csv')\n",
    "\n",
    "# Country Codes\n",
    "df_alpha2countrycode = pd.read_csv('inputs/country-codes2.csv', sep='|')\n",
    "\n",
    "# World Cities\n",
    "df_wc = pd.read_csv('inputs/worldcities.csv', sep=',')\n",
    "\n",
    "# US States\n",
    "df_USstatecode = pd.read_csv('inputs/us-states.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create copies of dataframe\n",
    "df_visatype1 = df_visatype.copy()\n",
    "df_ac1 = df_ac.copy()\n",
    "df_USPoE1 = df_USPoE.copy()\n",
    "df_uszips1 = df_uszips.copy()\n",
    "df_i94countrycode1 = df_i94countrycode.copy()\n",
    "df_uscd1 = df_uscd.copy()\n",
    "df_temper1 = df_temper.copy()\n",
    "df_wc1 = df_wc.copy()\n",
    "\n",
    "# Running strip() on all string columns\n",
    "df_i94countrycode1 =strip_all_columns(df_i94countrycode1).copy()\n",
    "df_USPoE1 =strip_all_columns(df_USPoE1).copy()\n",
    "df_temper1 = strip_all_columns(df_temper1).copy()\n",
    "df_visatype1 = strip_all_columns(df_visatype1).copy()\n",
    "df_wc1 = strip_all_columns(df_wc1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_i94countrycode : Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Update mexico\n",
    "temp = ['MEXICO Air Sea, and Not Reported (I-94, no land arrivals)']\n",
    "df_i94countrycode1.loc[df_i94countrycode1['country'].isin(temp), 'country']='MEXICO'\n",
    "# Below replace was not working\n",
    "#df_i94countrycode1['country'] = df_i94countrycode1['country'].str.replace('MEXICO Air Sea, and Not Reported (I-94, no land arrivals)','MEXICO')\n",
    "\n",
    "# Remove invalid codes / Collapsed / No Country\n",
    "df_i94countrycode1 = df_i94countrycode1[~df_i94countrycode1.country.str.lower().str.contains('invalid')]\n",
    "df_i94countrycode1 = df_i94countrycode1[~df_i94countrycode1.country.str.lower().str.contains('no country')]\n",
    "df_i94countrycode1 = df_i94countrycode1[~df_i94countrycode1.country.str.lower().str.contains('collapsed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_USPoE : Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_USPoE : Split one column into two\n",
    "df_USPoE1[\"citystate\"] = df_USPoE1[\"citystate\"].map(str.strip)\n",
    "\n",
    "df_USPoE1[['city', 'state']] = df_USPoE1['citystate'].str.rsplit(\",\",n=1, expand=True)\n",
    "df_USPoE1 = strip_all_columns(df_USPoE1)\n",
    "# trim all columns\n",
    "#for col in df_USPoE1.columns:\n",
    "#    df_USPoE1[col] = df_USPoE1[col].str.strip()\n",
    "\n",
    "# df_USPoE : Correct incorrect state code values\n",
    "df_USPoE1 = strip_all_columns(df_USPoE1).copy()\n",
    "#df_USPoE2['state'] = df_USPoE2['state'].str.replace('\\(BPS\\)', '')\n",
    "df_USPoE1['state'] = df_USPoE1.state.str.replace(r'\\(.*\\)$', '')\n",
    "df_USPoE1['state'] = df_USPoE1['state'].str.replace('#ARPT', '')\n",
    "df_USPoE1['state'] = df_USPoE1['state'].str.replace('#INTL', '')\n",
    "df_USPoE1['state'] = df_USPoE1['state'].str.replace('WASHINGTON', 'WA')\n",
    "df_USPoE1['state'] = df_USPoE1['state'].str.replace('MX', 'MEXICO')\n",
    "\n",
    "df_USPoE1['city'] = df_USPoE1['city'].str.replace('#ARPT', '')\n",
    "df_USPoE1['city'] = df_USPoE1['city'].str.replace('-ARPT', '')\n",
    "df_USPoE1['city'] = df_USPoE1['city'].str.replace('ARPT', '')\n",
    "df_USPoE1['city'] = df_USPoE1['city'].str.replace('INTL', '')\n",
    "df_USPoE1 = strip_all_columns(df_USPoE1).copy()\n",
    "\n",
    "# df_USPoE : Remove invalid rows & ports outside US\n",
    "df_USPoE1 = strip_all_columns(df_USPoE1).copy()\n",
    "df_USPoE1 = df_USPoE1[df_USPoE1.state.notnull()]\n",
    "\n",
    "cond1 = df_USPoE1.city.str.lower().str.contains('collapsed')\n",
    "df_USPoE1 = df_USPoE1[~cond1]\n",
    "\n",
    "cond1 = df_USPoE1.city.str.lower().str.contains('no port')\n",
    "df_USPoE1 = df_USPoE1[~cond1]\n",
    "\n",
    "cond1 = df_USPoE1.city.str.lower().str.contains('unknown')\n",
    "df_USPoE1 = df_USPoE1[~cond1]\n",
    "\n",
    "cond1 = df_USPoE1.city.str.lower().str.contains('identifi')\n",
    "df_USPoE1 = df_USPoE1[~cond1]\n",
    "\n",
    "df_USPoE1 = df_USPoE1[df_USPoE1.state.str.len() == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_ac : Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_ac : Keeping only US Airports which serves passenger flights and has customs & immigration\n",
    "# Dropping unused columns gps_code, ident\n",
    "drop_cols = ['gps_code', 'ident']\n",
    "df_ac1.drop(drop_cols, inplace = True, axis = 1)\n",
    "\n",
    "#Keep only US Airports\n",
    "df_ac1 = df_ac1[df_ac1.iso_country =='US'].copy()\n",
    "\n",
    "# Correcting wrong continent value \n",
    "cond1 = df_ac1.iso_country =='US'\n",
    "cond4 = df_ac1['continent'].isnull()\n",
    "ix = df_ac1[cond1 & ~cond4].index.values[0]\n",
    "df_ac1.at[ix,'continent']=np.nan\n",
    "\n",
    "# Eliminate rows which has iata code as null\n",
    "df_ac1 = df_ac1[~df_ac1.iata_code.isnull()].copy()\n",
    "\n",
    "# Keep only airport which is of type small_airport, medium_airport & large_airport\n",
    "temp = ['small_airport', 'medium_airport', 'large_airport']\n",
    "df_ac1 = df_ac1[df_ac1['type'].isin(temp)].copy()\n",
    "\n",
    "# Split geo-coordinates into separate columns\n",
    "df_ac1[['longitude','latitude']] = df_ac1.coordinates.str.split(\", \",expand=True)\n",
    "\n",
    "# Extract state code from iso_region\n",
    "df_ac1['state'] = df_ac1.iso_region.str.slice(start=3)\n",
    "\n",
    "# Add new column facilities to indicate whether airport has Customs & Immigration\n",
    "for code in df_iap['IATA']:\n",
    "    #print(code)    \n",
    "    df_ac1.loc[df_ac1['iata_code'] == code, 'facilities'] = 'CI'\n",
    "\n",
    "#Keeping only airports which has Customs & Immigration\n",
    "df_ac1 = df_ac1[df_ac1.facilities.notnull()].copy()\n",
    "    \n",
    "# Removing rows having 'Duplicate' in airport names\n",
    "df_ac1 = df_ac1[~df_ac1.name.str.contains('uplicate')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_ids : Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 28 - ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
      "1. 28 - ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat\n",
      "2. 28 - ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat\n",
      "3. 28 - ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat\n",
      "4. 34 - ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\n",
      "5. 28 - ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat\n",
      "6. 28 - ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat\n",
      "7. 28 - ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat\n",
      "8. 28 - ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\n",
      "9. 28 - ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat\n",
      "10. 28 - ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat\n",
      "11. 28 - ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# Checking files in the directory\n",
    "data_dir = \"../../data/18-83510-I94-Data-2016/\"\n",
    "files = os.listdir(data_dir)\n",
    "for i in range(len(files)):\n",
    "    files[i] = data_dir + files[i] \n",
    "    df_temp = spark.read.format('com.github.saurfang.sas.spark').load(files[i])\n",
    "    print('{}. {} - {}'.format(i, len(df_temp.columns), files[i]))\n",
    "#files\n",
    "#jun16 has 34 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 950 µs, sys: 238 µs, total: 1.19 ms\n",
      "Wall time: 106 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dfs_ids =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def to_datetime(x):\n",
    "    try:\n",
    "        start = dt.datetime(1960, 1, 1).date()\n",
    "        return start + dt.timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_to_datetime_sas = udf(lambda x: to_datetime(x), DateType())\n",
    "\n",
    "\n",
    "def to_datetimefrstr(x):\n",
    "    try:\n",
    "        return dt.datetime.strptime(x, '%m%d%Y')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "udf_to_datetimefrstr = udf(lambda x: to_datetimefrstr(x), DateType())\n",
    "\n",
    "def cdf_Ymd_to_mmddYYYY(x):\n",
    "    try:\n",
    "        return dt.datetime.strptime(x, '%Y%m%d')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "udf_cdf_Ymd_to_mmddYYYY = udf(lambda x: cdf_Ymd_to_mmddYYYY(x), DateType())\n",
    "\n",
    "def cdf_mdY_to_mmddYYYY(x):\n",
    "    try:\n",
    "        return dt.datetime.strptime(x, '%m%d%Y')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "udf_cdf_mdY_to_mmddYYYY = udf(lambda x: cdf_mdY_to_mmddYYYY(x), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs_ids1 = dfs_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Convert from pandas to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs_i94countrycode1 = spark.createDataFrame(df_i94countrycode1)\n",
    "dfs_USPoE1 = spark.createDataFrame(df_USPoE1)\n",
    "dfs_ac1 = spark.createDataFrame(df_ac1)\n",
    "\n",
    "ac_schema = StructType([\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"elevation_ft\", FloatType()),\n",
    "    StructField(\"continent\", StringType()),\n",
    "    StructField(\"iso_country\", StringType()),\n",
    "    StructField(\"iso_region\", StringType()),\n",
    "    StructField(\"muncipality\", StringType()),\n",
    "    StructField(\"iata_code\", StringType()),\n",
    "    StructField(\"local_code\", StringType()),\n",
    "    StructField(\"coordinates\", StringType()),   \n",
    "    StructField(\"longitude\", StringType()),   \n",
    "    StructField(\"latitude\", StringType()),   \n",
    "    StructField(\"state\", StringType()),   \n",
    "    StructField(\"facilities\", StringType()),   \n",
    "])\n",
    "dfs_ac1 = spark.createDataFrame(df_ac1, ac_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# matflag is null\n",
    "dfs_ids1 = dfs_ids1.filter(dfs_ids1.matflag.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# visatype GMT  \n",
    "temp = df_visatype1.visatype.tolist()\n",
    "dfs_ids1 = dfs_ids1.filter( dfs_ids1.visatype.isin(temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i94mode other than 1 2 3\n",
    "temp = [1, 2, 3]\n",
    "dfs_ids1 = dfs_ids1.filter( dfs_ids1.i94mode.isin(temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# gender is null\n",
    "dfs_ids1 = dfs_ids1.filter(dfs_ids1.gender.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Remove rows having invalid CoC & CoR\n",
    "temp = df_i94countrycode1.code.astype('int').tolist()\n",
    "dfs_ids1 = dfs_ids1.filter( dfs_ids1.i94cit.isin(temp) )\n",
    "dfs_ids1 = dfs_ids1.filter( dfs_ids1.i94res.isin(temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 85.8 ms, sys: 14.3 ms, total: 100 ms\n",
      "Wall time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Remove Non-US Port of entry data\n",
    "df1 = dfs_USPoE1.select('code')\n",
    "df2 = dfs_ids1.select('i94port')\n",
    "temp = df2.subtract(df1) #.collect()\n",
    "tempArr = [row.i94port for row in temp.collect()]\n",
    "dfs_ids1 = dfs_ids1.filter(~dfs_ids1.i94port.isin(tempArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total US States that has Customs & Immigration :  41\n"
     ]
    }
   ],
   "source": [
    "temp = dfs_ac1.filter( dfs_ac1.facilities.isNotNull() ).select('state').dropDuplicates()\n",
    "df1 = [row.state for row in temp.collect()]\n",
    "print('Total US States that has Customs & Immigration : ',len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total US States that has Customs & Immigration :  41\n",
      "Number of Port of Entries that needs to be deleted from df_USPoE :  105\n",
      "CPU times: user 128 ms, sys: 6.24 ms, total: 134 ms\n",
      "Wall time: 5.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This takes 2 mins due to count() without it takes 3.36s\n",
    "# Delete Port of Entries which lacks International Passenger Handling Facility operated by US Customs.\n",
    "# df_ac has to be processed after df_USPoE1 and before df_ids1\n",
    "temp = dfs_ac1.filter( dfs_ac1.facilities.isNotNull() ).select('state').dropDuplicates()\n",
    "df1 = [row.state for row in temp.collect()]\n",
    "print('Total US States that has Customs & Immigration : ',len(df1))\n",
    "temp = dfs_USPoE1.filter(~dfs_USPoE1.state.isin(df1)).select('code').dropDuplicates()\n",
    "df2 = [row.code for row in temp.collect()]\n",
    "print('Number of Port of Entries that needs to be deleted from df_USPoE : ',len(df2))\n",
    "#print('Total rows that will be deleted from df_ids due to PoE that doesnt have Customs & Immigration : '\n",
    "#      ,dfs_ids1.filter( dfs_ids1.i94port.isin(df2)).count())\n",
    "#print('Before : dfs_ids1 = ',dfs_ids1.count())\n",
    "dfs_ids1 = dfs_ids1.filter(~dfs_ids1.i94port.isin(df2))\n",
    "#print('After  : dfs_ids1 = ',dfs_ids1.count())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cicid', 'i94yr', 'i94mon', 'count', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'insnum']\n"
     ]
    }
   ],
   "source": [
    "# Dropping unused columns\n",
    "# Only below columns are used for analysis\n",
    "keep_columns = ['i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate'\n",
    "                , 'i94bir', 'i94visa', 'dtadfile', 'visapost', 'occup', 'biryear'\n",
    "                , 'dtaddto', 'gender', 'airline', 'admnum', 'fltno', 'visatype']\n",
    "drop_cols = difference(keep_columns, dfs_ids1.columns)\n",
    "print(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs_ids1 = dfs_ids1.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#data_df = data_df.withColumn(\"Plays\", data_df[\"Plays\"].cast(IntegerType()))\n",
    "# Convert floats to ints\n",
    "cols_to_convert_float_to_integer = ['i94cit', 'i94res', 'arrdate', 'i94mode', 'depdate', 'i94bir'\n",
    "                                , 'i94visa', 'biryear', 'admnum']\n",
    "for colu in cols_to_convert_float_to_integer:    \n",
    "    dfs_ids1 = dfs_ids1.na.fill(0, subset=[colu])\n",
    "    dfs_ids1 = dfs_ids1.withColumn(colu, dfs_ids1[colu].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Mapping : Codes to descriptive\n",
    "#temp = {'1' : 'Air', '2' : 'Sea', '3' : 'Land', '9' : 'Not reported'}\n",
    "temp = [[\"1\", \"Air\"], [\"2\", \"Sea\"], [\"3\",\"Land\"], [\"9\", \"Not reported\"]]\n",
    "i94mode = spark.sparkContext.parallelize(temp).toDF([\"code\", \"arrival_mode\"])\n",
    "dfs_ids1 = dfs_ids1.join(i94mode, dfs_ids1.i94mode == i94mode.code).select(dfs_ids1[\"*\"], i94mode[\"arrival_mode\"])\n",
    "\n",
    "temp = [[\"1\", \"Business\"], [\"2\", \"Pleasure\"], [\"3\", \"Student\"]]\n",
    "i94visa = spark.sparkContext.parallelize(temp).toDF([\"code\", \"visit_purpose\"])\n",
    "dfs_ids1 = dfs_ids1.join(i94visa, dfs_ids1.i94visa == i94visa.code).select(dfs_ids1[\"*\"], i94visa[\"visit_purpose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import *\n",
    "# Conversion of SAS encoded dates(arrdate & depdate)\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"arrival_dt\", udf_to_datetime_sas(dfs_ids1.arrdate))\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"departure_dt\", udf_to_datetime_sas(dfs_ids1.depdate))\n",
    "\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"DaysinUS\", datediff(\"departure_dt\", \"arrival_dt\"))\n",
    "\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"added_to_i94\", udf_cdf_Ymd_to_mmddYYYY(dfs_ids1.dtadfile))\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"allowed_until\", udf_cdf_mdY_to_mmddYYYY(dfs_ids1.dtaddto))\n",
    "\n",
    "# Below corrections are carried out due to above adding 1960-01-01\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"arrival_dt\",when(col(\"arrival_dt\")==\"1960-01-01\",lit(None)).otherwise(col(\"arrival_dt\")))\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"departure_dt\",when(col(\"departure_dt\")==\"1960-01-01\",lit(None)).otherwise(col(\"departure_dt\")))\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"DaysinUS\",when(col(\"arrival_dt\").isNull(),lit(None)).otherwise(col(\"DaysinUS\")))\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"DaysinUS\",when(col(\"departure_dt\").isNull(),lit(None)).otherwise(col(\"DaysinUS\")))\n",
    "\n",
    "# Departure date can't before Arrival date \n",
    "# ~(arrival date > departure date ) or (departure date can be null)\n",
    "dfs_ids1 = dfs_ids1.filter(~(dfs_ids1.arrival_dt > dfs_ids1.departure_dt) | (dfs_ids1.departure_dt.isNull()))\n",
    "\n",
    "# Change date format to YYYY-mm-dd\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"added_to_i94\", udf_cdf_Ymd_to_mmddYYYY(dfs_ids1.dtadfile))\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"allowed_until\", udf_cdf_mdY_to_mmddYYYY(dfs_ids1.dtaddto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function _create_function.<locals>._ at 0x7f96a082b840>\n",
      "<function when at 0x7f96a07dcea0>\n",
      "<function _create_function.<locals>._ at 0x7f96a082b7b8>\n"
     ]
    }
   ],
   "source": [
    "print(col)\n",
    "print(when)\n",
    "print(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Columns Rename\n",
    "dfs_ids1 = (dfs_ids1\n",
    "            .withColumnRenamed(\"i94bir\",  \"age\")\n",
    "            .withColumnRenamed(\"i94cit\", \"CoC\")\n",
    "            .withColumnRenamed(\"i94res\", \"CoR\")\n",
    "            .withColumnRenamed(\"i94port\", \"PoE\")\n",
    "            .withColumnRenamed(\"i94addr\", \"landing_state\")\n",
    "            .withColumnRenamed(\"visapost\", \"visa_issued_in\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Entry_Exit\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"entry_exit\",when(col(\"departure_dt\").isNull(),lit(\"entry\")).otherwise(lit(\"exit\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#dfs_ids1.groupby(dfs_ids1.entry_exit).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Gender X to O\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"gender\", when(col(\"gender\")==\"X\", lit(\"O\")).otherwise(col(\"gender\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Final Drop of unused columns\n",
    "drop_cols = ['i94mode', 'i94visa', 'arrdate', 'depdate', 'dtadfile', 'dtaddto']\n",
    "#print(drop_cols)\n",
    "dfs_ids1 = dfs_ids1.drop(*drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Adding month which is used when saving file in parquet format partioning by month & landing state\n",
    "dfs_ids1 = dfs_ids1.withColumn(\"month\", month(\"arrival_dt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Lots of displays\n",
    "#dfs_ids1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs_ids2 = dfs_ids1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs_ids1 = dfs_ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CoC: integer (nullable = true)\n",
      " |-- CoR: integer (nullable = true)\n",
      " |-- PoE: string (nullable = true)\n",
      " |-- landing_state: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- visa_issued_in: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- arrival_mode: string (nullable = true)\n",
      " |-- visit_purpose: string (nullable = true)\n",
      " |-- arrival_dt: date (nullable = true)\n",
      " |-- departure_dt: date (nullable = true)\n",
      " |-- DaysinUS: integer (nullable = true)\n",
      " |-- added_to_i94: date (nullable = true)\n",
      " |-- allowed_until: date (nullable = true)\n",
      " |-- entry_exit: string (nullable = false)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_ids1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_visatype : Strip space from code column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_visatype1[\"visatype\"] = df_visatype1[\"visatype\"].map(str.strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_uszips : drop unused columns (zcta, parent_zcta, all_county_weights, imprecise, military, timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_cols = ['zcta', 'parent_zcta', 'all_county_weights', 'imprecise', 'military', 'timezone']\n",
    "df_uszips1.drop(drop_cols, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_uscd : Updates to demographic dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_uscd1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convert floats to ints\n",
    "cols_to_convert_float_to_int = ['Male Population', 'Female Population', 'Number of Veterans', 'Foreign-born', 'Total Population', 'Count']\n",
    "\n",
    "for col in cols_to_convert_float_to_int:\n",
    "    df_uscd1[col] = df_uscd1[col].replace(np.nan, 0)\n",
    "    df_uscd1[col] = df_uscd1[col].astype(int)\n",
    "\n",
    "# Keeping only rows where MP + FP = TP \n",
    "df_uscd1 = df_uscd1[(df_uscd1['Male Population'] + df_uscd1['Female Population'] == df_uscd1['Total Population'])].copy()\n",
    "    \n",
    "# US : Cities check to if any names are misspelled\n",
    "df1 = df_uszips[['city']]\n",
    "lst = df1.city.str.lower().unique()\n",
    "test = df_uscd1[~df_uscd1.City.str.lower().isin(lst)]\n",
    "\n",
    "# removing rows of misspelled cities from df_uscd1 to process separately\n",
    "df_uscd1 = df_uscd1[df_uscd1.City.str.lower().isin(lst)]\n",
    "\n",
    "test1 = test.copy()\n",
    "\n",
    "# Performing fuzzy match against the city rows which were not found in df_uszips\n",
    "test2 = test1.apply(fuzzymatch_city_get_ratio, axis=1)\n",
    "test2.columns = ['new_city_name','score']\n",
    "test1 = test1.join(test2)\n",
    "\n",
    "# Updating city names with score >= 80\n",
    "test1['City'] = test1.apply(lambda x: x['new_city_name'] if x.score >= 80 else x['City'], axis=1)\n",
    "\n",
    "# Convert lowercase city names to title case as before\n",
    "test1.City = test1.City.str.strip().str.title()\n",
    "\n",
    "# Dropping columns in fuzzy match dataframes\n",
    "drop_cols = ['new_city_name', 'score']\n",
    "test1.drop(drop_cols, inplace = True, axis = 1)\n",
    "\n",
    "# Concatinating all the dataframes 1\n",
    "test3 = [df_uscd1, test1]\n",
    "df_uscd1 = pd.concat(test3).copy()\n",
    "\n",
    "# Merging duplicates by avging & summing\n",
    "temp = df_uscd1[df_uscd1.City=='Los Angeles'].copy()\n",
    "df_uscd1 = df_uscd1[df_uscd1.City!='Los Angeles'].copy()\n",
    "temp = temp.groupby(['City','State', 'State Code', 'Race']).agg({'Median Age':'mean','Male Population':'sum', 'Female Population': 'sum'\n",
    "                                                          , 'Number of Veterans':'sum', 'Foreign-born':'sum'\n",
    "                                                          , 'Average Household Size':'mean', 'Count':'sum' }).reset_index().copy()\n",
    "# Concatinating all the dataframes 2\n",
    "frames = [df_uscd1, temp]\n",
    "# Encountered this warning while doing pd.concat : FutureWarning: Sorting because non-concatenation axis is not aligned. A future version of pandas will change to not sort by default.\n",
    "df_uscd1 = pd.concat(frames, sort=False).copy()\n",
    "\n",
    "# Above concat process changes the total population to float, so converting it back to Int\n",
    "df_uscd1['Total Population'] = df_uscd1['Total Population'].replace(np.nan, 0)\n",
    "df_uscd1['Total Population'] = df_uscd1['Total Population'].astype(int)\n",
    "\n",
    "# Split based on Gender\n",
    "df_usdp = df_uscd1[['State', 'City', 'Median Age', 'State Code', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born']]\n",
    "df_usdp = df_usdp.drop_duplicates()\n",
    "\n",
    "# Split based on Race\n",
    "df_usdr = df_uscd1[['State', 'City', 'State Code', 'Race', 'Count']].copy()\n",
    "\n",
    "# Summing up the duplicates(ie., East Los Angeles -> Los Angeles)\n",
    "df_usdr = df_usdr.groupby(['State', 'City', 'State Code', 'Race']).sum().reset_index().copy()\n",
    "\n",
    "# Unmelting to converts Race rows to columns\n",
    "df_usdr = df_usdr.set_index(['State','City','State Code', 'Race']).Count.unstack().reset_index()\n",
    "df_usdr.columns.name = None\n",
    "df_usdr[df_usdr.State =='Texas'].head()\n",
    "\n",
    "# Convert floats to ints\n",
    "cols_to_convert_float_to_int = ['American Indian and Alaska Native','Asian', 'Black or African-American', 'Hispanic or Latino', 'White']\n",
    "\n",
    "for col in cols_to_convert_float_to_int:\n",
    "    df_usdr[col] = df_usdr[col].replace(np.nan, 0)\n",
    "    df_usdr[col] = df_usdr[col].astype(int)\n",
    "    \n",
    "# Rename column names\n",
    "df_usdr.rename(columns={\"State Code\":\"State_Code\", \"American Indian and Alaska Native\": \"American_Indian_and_Alaska_Native\"\n",
    "                        , \"Black or African-American\":\"Black_or_African_American\"\n",
    "                        , \"Hispanic or Latino\":\"Hispanic_or_Latino\"}, inplace=True)\n",
    "\n",
    "df_usdp.rename(columns={ \"Median Age\": \"Median_Age\" , \"State Code\":\"State_Code\", \"Male_Population\":\"Male_Population\"\n",
    "                         , \"Female Population\":\"Female_Population\", \"Total Population\":\"Total_Population\" \n",
    "                         , \"Number of Veterans\":\"Number_of_Veterans\", \"Foreign-born\" : \"Foreign_born\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_temp : Updates to weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Keeping only USA\n",
    "df_temper1 = df_temper1[df_temper1.Country=='United States'].copy()\n",
    "\n",
    "# Rounding temperature to decimals=3 \n",
    "df_temper1['AverageTemperature'] = df_temper1['AverageTemperature'].round(decimals=3)\n",
    "\n",
    "# Dropping unused columns\n",
    "drop_cols = ['AverageTemperatureUncertainty', 'Latitude', 'Longitude']\n",
    "df_temper1.drop(drop_cols, inplace = True, axis = 1)\n",
    "\n",
    "# Removing missing temperatures\n",
    "df_temper1 = df_temper1[~(df_temper1.AverageTemperature.isnull())]\n",
    "\n",
    "# Eliminating the duplicates(ie., multiple locations in same city)\n",
    "df_temper1 = df_temper1.drop_duplicates(['dt', 'City', 'Country'],keep= 'first')\n",
    "\n",
    "# Convert dt to datetime from object\n",
    "df_temper1['dt'] = pd.to_datetime(df_temper1.dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### df_wc : Updates to world cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_wc1['population'] = df_wc1['population'].replace(np.nan, 0)\n",
    "df_wc1['population'] = df_wc1['population'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Store\n",
    "##### Save clean dataframe to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned datasets\n",
      "df_visatype1 = 108\n",
      "df_ac1 = 113\n",
      "df_USPoE1 = 526\n",
      "df_uszips1 = 33099\n",
      "df_i94countrycode1 = 236\n",
      "df_uscd1 = 2883\n",
      "df_usdp = 594\n",
      "df_usdr = 594\n",
      "df_temper1 = 639649\n",
      "df_wc1 = 15493\n",
      "Datasets with no changes/updates\n",
      "df_alc = 1571\n",
      "df_visapost = 284\n",
      "df_alpha2countrycode = 249\n",
      "df_iap = 1304\n",
      "df_USstatecode = 62\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned datasets\")\n",
    "#print(\"{} = {}\".format(\"df_ids1\", df_ids1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_visatype1\", df_visatype1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_ac1\", df_ac1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_USPoE1\", df_USPoE1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_uszips1\", df_uszips1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_i94countrycode1\", df_i94countrycode1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_uscd1\", df_uscd1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_usdp\", df_usdp.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_usdr\", df_usdr.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_temper1\", df_temper1.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_wc1\", df_wc1.shape[0]))\n",
    "\n",
    "print(\"Datasets with no changes/updates\")\n",
    "print(\"{} = {}\".format(\"df_alc\", df_alc.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_visapost\", df_visapost.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_alpha2countrycode\", df_alpha2countrycode.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_iap\", df_iap.shape[0]))\n",
    "print(\"{} = {}\".format(\"df_USstatecode\", df_USstatecode.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cleaned datasets\n",
    "#df_ids1.to_csv('outputs/df_ids1.csv', encoding='utf-8', index=False)\n",
    "#dfs_ids1.write.csv('outputs/dfs_ids1_i94_apr16_sub.csv')\n",
    "df_visatype1.to_csv('outputs/df_visatype1.csv', encoding='utf-8', index=False)\n",
    "df_ac1.to_csv('outputs/df_ac1.csv', encoding='utf-8', index=False)\n",
    "df_USPoE1.to_csv('outputs/df_USPoE1.csv', encoding='utf-8', index=False)\n",
    "df_uszips1.to_csv('outputs/df_uszips1.csv', encoding='utf-8', index=False)\n",
    "df_i94countrycode1.to_csv('outputs/df_i94countrycode1.csv', encoding='utf-8', index=False)\n",
    "df_uscd1.to_csv('outputs/df_uscd1.csv', encoding='utf-8', index=False)\n",
    "df_usdp.to_csv('outputs/df_usdp.csv', encoding='utf-8', index=False)\n",
    "df_usdr.to_csv('outputs/df_usdr.csv', encoding='utf-8', index=False)\n",
    "df_temper1.to_csv('outputs/df_temper1.csv', encoding='utf-8', index=False)\n",
    "df_wc1.to_csv('outputs/df_wc1.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Below datasets have not changes/updates, just writing to another folder\n",
    "df_alc.to_csv('outputs/df_alc.csv', encoding='utf-8', index=False)\n",
    "df_visapost.to_csv('outputs/df_visapost.csv', encoding='utf-8', index=False)\n",
    "df_alpha2countrycode.to_csv('outputs/df_alpha2countrycode.csv', encoding='utf-8', index=False)\n",
    "df_iap.to_csv('outputs/df_iap.csv', encoding='utf-8', index=False)\n",
    "df_USstatecode.to_csv('outputs/df_USstatecode.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Successfully creates partitioned parquet - But i am getting errors while loading into Redshift from S3\n",
    "#dfs_ids1.write.parquet('./outputs-parquet/i94-apr16.parquet', mode='overwrite', partitionBy=['month', 'landing_state'])    \n",
    "\n",
    "#Parquet format output doesn't have null value fields due to which not able to load into redshift\n",
    "#dfs_ids1.write.parquet('./outputs-parquet/i94-apr16.parquet', mode='overwrite')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_aws_keysecret(filepath):\n",
    "    with open(filepath) as myfile:\n",
    "        head = [next(myfile) for x in range(4)]\n",
    "    #print(head)\n",
    "    #print(head[3])\n",
    "    for s in head:\n",
    "        #print(\"KEYSECRET\" in s)\n",
    "        if \"KEYSECRET\" in s:\n",
    "            arr = s.split('KEYSECRET=')\n",
    "            key = arr[1]\n",
    "            #print(key)\n",
    "            break\n",
    "    return key.strip()\n",
    "\n",
    "awskey = get_aws_keysecret(\"./aws/aws-capstone.cfg\")\n",
    "#print(awskey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_copy_statement(table, data, awskey):\n",
    "    copy_stmt = (\"\"\"\n",
    "        COPY {} FROM {}\n",
    "        CREDENTIALS '{}'\n",
    "        TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL\n",
    "        CSV \n",
    "        delimiter ',' \n",
    "        IGNOREHEADER 1\n",
    "        COMPUPDATE OFF REGION 'us-west-2';\n",
    "        \"\"\").format(table, data, awskey)\n",
    "    return copy_stmt\n",
    "\n",
    "awskey = get_aws_keysecret(\"./aws/aws-capstone.cfg\")\n",
    "#print(make_copy_statement('staging_ids', 's3://asdf', awskey))\n",
    "a = {\n",
    "    \"dfs_ids1\":\"ds_ids1.csv\",\n",
    "    \"dfs_ids\":\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staging_visatype 's3://sushanth-dend-capstone-files/outputs/df_visatype1.csv'\n",
      "staging_ac 's3://sushanth-dend-capstone-files/outputs/df_ac1.csv'\n",
      "staging_USPoE 's3://sushanth-dend-capstone-files/outputs/df_USPoE1.csv'\n",
      "staging_uszips 's3://sushanth-dend-capstone-files/outputs/df_uszips1.csv'\n",
      "staging_i94countrycode 's3://sushanth-dend-capstone-files/outputs/df_i94countrycode1.csv'\n",
      "staging_usdp 's3://sushanth-dend-capstone-files/outputs/df_usdp.csv'\n",
      "staging_usdr 's3://sushanth-dend-capstone-files/outputs/df_usdr.csv'\n",
      "staging_temper 's3://sushanth-dend-capstone-files/outputs/df_temper1.csv'\n",
      "staging_alc 's3://sushanth-dend-capstone-files/outputs/df_alc.csv'\n",
      "staging_visapost 's3://sushanth-dend-capstone-files/outputs/df_visapost.csv'\n",
      "staging_alpha2countrycode 's3://sushanth-dend-capstone-files/outputs/df_alpha2countrycode.csv'\n",
      "staging_iap 's3://sushanth-dend-capstone-files/outputs/df_iap.csv'\n",
      "staging_wc 's3://sushanth-dend-capstone-files/outputs/df_wc1.csv'\n",
      "staging_USstatecode 's3://sushanth-dend-capstone-files/outputs/df_USstatecode.csv'\n"
     ]
    }
   ],
   "source": [
    "fname = 'inputs/staging-table-data.txt'\n",
    "table_data = json.load(open(fname))\n",
    "#print(table_data[\"staging_uspoe\"])\n",
    "\n",
    "for table, data in table_data.items():\n",
    "    print(table, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./sas_inputs/i94_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', './sas_inputs/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|year|month|day|\n",
      "+----+-----+---+\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "|1984|    1|  1|\n",
      "+----+-----+---+\n",
      "\n",
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|    1|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "elevDF = spark.sparkContext.parallelize([\n",
    "    (datetime.datetime(1984, 1, 1, 0, 0), 1, 638.55),\n",
    "    (datetime.datetime(1984, 1, 1, 0, 0), 2, 638.55),\n",
    "    (datetime.datetime(1984, 1, 1, 0, 0), 3, 638.55),\n",
    "    (datetime.datetime(1984, 1, 1, 0, 0), 4, 638.55),\n",
    "    (datetime.datetime(1984, 1, 1, 0, 0), 5, 638.55)\n",
    "]).toDF([\"date\", \"hour\", \"value\"])\n",
    "\n",
    "elevDF.select(\n",
    "    year(\"date\").alias('year'), \n",
    "    month(\"date\").alias('month'), \n",
    "    dayofmonth(\"date\").alias('day')\n",
    ").show()\n",
    "\n",
    "\n",
    "#temp = dfs_ac1.filter( dfs_ac1.facilities.isNotNull() ).select('state').dropDuplicates()\n",
    "elevDF.select(\n",
    "    month(\"date\").alias('month')\n",
    "    ).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CoC: integer (nullable = true)\n",
      " |-- CoR: integer (nullable = true)\n",
      " |-- PoE: string (nullable = true)\n",
      " |-- landing_state: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- visa_issued_in: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- arrival_mode: string (nullable = true)\n",
      " |-- visit_purpose: string (nullable = true)\n",
      " |-- arrival_dt: date (nullable = true)\n",
      " |-- departure_dt: date (nullable = true)\n",
      " |-- DaysinUS: integer (nullable = true)\n",
      " |-- added_to_i94: date (nullable = true)\n",
      " |-- allowed_until: date (nullable = true)\n",
      " |-- entry_exit: string (nullable = false)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_ids1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CoC',\n",
       " 'CoR',\n",
       " 'PoE',\n",
       " 'landing_state',\n",
       " 'age',\n",
       " 'visa_issued_in',\n",
       " 'occup',\n",
       " 'biryear',\n",
       " 'gender',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype',\n",
       " 'arrival_mode',\n",
       " 'visit_purpose',\n",
       " 'arrival_dt',\n",
       " 'departure_dt',\n",
       " 'DaysinUS',\n",
       " 'added_to_i94',\n",
       " 'allowed_until',\n",
       " 'entry_exit',\n",
       " 'month']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_ids1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2093913"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_ids1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dfs_ids1.write\\\n",
    "  .format(\"com.databricks.spark.csv\")\\\n",
    "  .option(\"header\",\"true\")\n",
    "  .option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    "  .save('./outputs-gzip/dfs_ids1.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 28 - ../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat : ./sas_inputs/i94_apr16_sub.sas7bdat\n",
      "1. 28 - ../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat : ./sas_inputs/i94_sep16_sub.sas7bdat\n",
      "2. 28 - ../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat : ./sas_inputs/i94_nov16_sub.sas7bdat\n",
      "3. 28 - ../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat : ./sas_inputs/i94_mar16_sub.sas7bdat\n",
      "4. 34 - ../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat : ./sas_inputs/i94_jun16_sub.sas7bdat\n",
      "5. 28 - ../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat : ./sas_inputs/i94_aug16_sub.sas7bdat\n",
      "6. 28 - ../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat : ./sas_inputs/i94_may16_sub.sas7bdat\n",
      "7. 28 - ../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat : ./sas_inputs/i94_jan16_sub.sas7bdat\n",
      "8. 28 - ../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat : ./sas_inputs/i94_oct16_sub.sas7bdat\n",
      "9. 28 - ../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat : ./sas_inputs/i94_jul16_sub.sas7bdat\n",
      "10. 28 - ../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat : ./sas_inputs/i94_feb16_sub.sas7bdat\n",
      "11. 28 - ../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat : ./sas_inputs/i94_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# Checking files in the directory\n",
    "data_dir = \"../../data/18-83510-I94-Data-2016/\"\n",
    "files = os.listdir(data_dir)\n",
    "for i in range(len(files)):\n",
    "    files[i] = data_dir + files[i] \n",
    "    df_temp = spark.read.format('com.github.saurfang.sas.spark').load(files[i])\n",
    "    file = files[i].split('/')\n",
    "    print('{}. {} - {} : {}'.format(i, len(df_temp.columns), files[i], './sas_inputs/'+file[-1]))\n",
    "#files\n",
    "#jun16 has 34 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat', '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat']\n"
     ]
    }
   ],
   "source": [
    "# Checking files in the directory\n",
    "data_dir = \"../../data/18-83510-I94-Data-2016/\"\n",
    "files = os.listdir(data_dir)\n",
    "sas_files = [data_dir + files[i] for i in range(len(files))]    \n",
    "print(sas_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CoC: integer (nullable = true)\n",
      " |-- CoR: integer (nullable = true)\n",
      " |-- PoE: string (nullable = true)\n",
      " |-- landing_state: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- visa_issued_in: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- biryear: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: integer (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- arrival_mode: string (nullable = true)\n",
      " |-- visit_purpose: string (nullable = true)\n",
      " |-- arrival_dt: date (nullable = true)\n",
      " |-- departure_dt: date (nullable = true)\n",
      " |-- DaysinUS: integer (nullable = true)\n",
      " |-- added_to_i94: date (nullable = true)\n",
      " |-- allowed_until: date (nullable = true)\n",
      " |-- entry_exit: string (nullable = false)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_ids1.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
